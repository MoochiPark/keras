# Part 02. 딥러닝 개념잡기



## Chapter 02. 학습과정 이야기

같은 문제집이라도 사람마다 푸는 방식과 학습된 결과가 다른 것처럼 딥러닝 모델의 학습도 비슷하다.
케라스에서는 모델을 학습시킬 때 `fit()` 함수를 사용하는데, 그 인자에 따라 학습 과정 및 결과가 다르다.



### 배치 사이즈와 에포크

`fit()` 함수의 학습 과정이 어떤 방식으로 일어나는 지 살펴보자.

```python
model.fit(x, y, batch_size=32, epochs=10)
```

- **x** : 입력 데이터
- **y** : 라벨 값
- **batch_size** : 몇 개의 샘플로 가중치를 갱신할 것인지 지정
- **epochs** : 학습 반복 횟수



위를 시험 공부하는 것에 비유해보자.

![image](https://user-images.githubusercontent.com/43429667/79726483-d774b500-8325-11ea-879a-3fefeb51d9da.png)

##### x

100문항의 문제들

##### y

100문항의 답들

##### batch_size

몇 문항을 풀고 해답을 맞추는 지를 의미한다. 이 과정을 통해 가중치 갱신이 일어난다.

> 문제를 푼 뒤 해답과 맞춰봐야 학습이 일어난다. 모델의 결과값과 주어진 라벨 값의 오차를 줄이기 위해
> 역전파<sup>Backpropagation</sup> 알고리즘으로 가중치가 갱신된다.
>
> 역전파 : 먼저 계산 결과와 정답의 오차를 구해 이 오차에 관여하는 값들의 가증치를 수정하여 오차가 작아지는 방향으로 일정 횟수를 반복해 수정하는 방법



##### 배치 사이즈 : 100

![image](https://user-images.githubusercontent.com/43429667/79727482-6d5d0f80-8327-11ea-9f7a-1a100d6a431e.png)

가중치 갱신 : 1회



##### 배치 사이즈 : 10

![image](https://user-images.githubusercontent.com/43429667/79727536-7fd74900-8327-11ea-93af-0b3e2375c8c5.png)

가중치 갱신 : 10회



##### 배치 사이즈 : 1

![image](https://user-images.githubusercontent.com/43429667/79727661-b0b77e00-8327-11ea-9b1c-01fc186a5c4b.png)

가중치 갱신 : 100회



### 큰 배치 사이즈 vs 작은 배치 사이즈

배치 사이즈와 학습효과와 어떤 관계가 있을까? 이것은 사람이 학습하는 것이랑 비슷하다.
100문항 다 풀고 해답과 맞추어보려면 문제가 무엇이있는지 다 기억을 해야 맞춰볼 것이다. 따라서 기억력<sup>용량</sup>이 커야한다.
1문항씩 풀고 해답을 맞추면 학습은 꼼꼼하게 되지만 시간이 너무 걸릴 것이다. 

> 배치사이즈가 작을수록 가중치 갱신이 자주 일어난다.





#### epochs<sup>에포크</sup>

에포크가 20이면 1회분을 20번 푸는 것이다. 같은 데이터셋으로도 반복적으로 가중치를 갱신하면서 모델이 학습된다.

> 같은 문제집이라도 반복해서 풀면 학습이 일어난다.

<img src="https://user-images.githubusercontent.com/43429667/79731840-cfb90e80-832d-11ea-9340-c14fd4d9aa25.png" alt="image" style="zoom:50%;" />

처음에는 틀린 개수가 확 적어지지만 반복이 늘어날수록 완만하게 틀린 개수가 줄어든다.

현실적으로 데이터를 구하기가 쉽지 않기 때문에 제한된 데이터셋으로 반복적으로 학습하는 것이 효율적이다.

그렇다고 에포크를 무조건 늘리는 것은 역효과가 일어날 수 있다. 이것을 **오버피팅**<sup>overfitting</sup>이라 한다.
실제로 모델을 학습할 때도 오버피팅의 조짐이 보이면 학습을 중단한다.